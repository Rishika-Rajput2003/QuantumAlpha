{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import alphalens as al\n",
    "import cvxpy as cp\n",
    "import itertools\n",
    "from datetime import datetime, timedelta\n",
    "import nasdaqdatalink\n",
    "\n",
    "nasdaqdatalink.read_key(filename=\"key\")\n",
    "\n",
    "# Step 1: Get S&P 500 Companies\n",
    "def get_sp500_tickers():\n",
    "    \"\"\"Fetches S&P 500 tickers from Wikipedia.\"\"\"\n",
    "    table = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\")[0]\n",
    "    return table[['Symbol', 'GICS Sector']].set_index('Symbol')\n",
    "\n",
    "# Step 2: Universe Selection\n",
    "def filter_universe(meta, tickers, exchange_select=None, currency_select=None, delisted_select=None, cap_select=None):\n",
    "    \"\"\"Filter stock universe based on exchange, currency, delisting status, and market cap.\"\"\"\n",
    "    \n",
    "    meta = meta.loc[tickers]  # Apply filtering only on S&P 500 tickers\n",
    "    \n",
    "    if exchange_select:\n",
    "        exchange = [list(meta[(meta['exchange'] == i)].index) for i in exchange_select]\n",
    "        meta_ex = meta.loc[list(itertools.chain.from_iterable(exchange))]\n",
    "    else:\n",
    "        meta_ex = meta\n",
    "\n",
    "\n",
    "    if currency_select:\n",
    "        currency = [list(meta_ex[(meta_ex['currency'] == i)].index) for i in currency_select]\n",
    "        meta_ex_cu = meta_ex.loc[list(itertools.chain.from_iterable(currency))]\n",
    "    else:\n",
    "        meta_ex_cu = meta_ex\n",
    "\n",
    "    if delisted_select:\n",
    "        delisted = [list(meta_ex_cu[(meta_ex_cu['isdelisted'] == i)].index) for i in delisted_select]\n",
    "        meta_ex_cu_de = meta_ex_cu.loc[list(itertools.chain.from_iterable(delisted))]\n",
    "    else:\n",
    "        meta_ex_cu_de = meta_ex_cu\n",
    "\n",
    "\n",
    "    if cap_select:\n",
    "        cap = [list(meta_ex_cu_de[(meta_ex_cu_de['scalemarketcap'] == i)].index) for i in cap_select]\n",
    "        meta_ex_cu_de_cap = meta_ex_cu_de.loc[list(itertools.chain.from_iterable(cap))]\n",
    "    else:\n",
    "        meta_ex_cu_de_cap = meta_ex_cu_de\n",
    "\n",
    "    return meta_ex_cu_de_cap.index.tolist(), meta_ex_cu_de_cap\n",
    "\n",
    "# Step 3: Fetch OHLCV Data\n",
    "def get_stock_data(tickers, start, end):\n",
    "    \"\"\"Fetch OHLCV data from Yahoo Finance, including High and Low prices.\"\"\"\n",
    "    data = yf.download(tickers, start=start, end=end)\n",
    "    return data  # Include High & Low\n",
    "\n",
    "def dollar_volume_universe(tickers_num, ohlcv, sma_period):\n",
    "    \"\"\"Filter stocks based on dollar volume.\"\"\"\n",
    "    \n",
    "    # Compute dollar volume for each ticker\n",
    "    dollar_vol = ohlcv['Close'] * ohlcv['Volume']\n",
    "    \n",
    "    # Convert to long format and take the moving average\n",
    "    dollar_vol_sma = dollar_vol.rolling(window=sma_period).mean()\n",
    "\n",
    "    # Get the latest dollar volume values\n",
    "    last_dv = dollar_vol_sma.iloc[-1, :]\n",
    "\n",
    "    # Create DataFrame for sorting\n",
    "    dol = pd.DataFrame({'dv': last_dv})\n",
    "    \n",
    "    # Drop NaN values\n",
    "    dol.dropna(inplace=True)\n",
    "\n",
    "    # Select top tickers based on dollar volume\n",
    "    return list(dol.sort_values(by='dv', ascending=False).iloc[:tickers_num].index)\n",
    "\n",
    "\n",
    "# Step 5: Sector Filtering\n",
    "def filter_by_sector(universe, meta_ex_cu_de_cap, sec_to_drop):\n",
    "    \"\"\"Filter stocks by removing specific sectors.\"\"\"\n",
    "    universe_sectors = pd.DataFrame(index=universe, columns=['sectors'])\n",
    "    for i in universe:\n",
    "        try:\n",
    "            universe_sectors.loc[i] = meta_ex_cu_de_cap.loc[i]['sector']\n",
    "        except:\n",
    "            universe_sectors.loc[i] = np.nan\n",
    "        try:\n",
    "            for sec in sec_to_drop:\n",
    "                if meta_ex_cu_de_cap.loc[i]['sector'] == sec:\n",
    "                    print(1)\n",
    "                    universe_sectors.drop(i, axis=0, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "    return universe_sectors.index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2020-01-01'\n",
    "end_date = '2024-01-01'\n",
    "\n",
    "sp500_tickers = get_sp500_tickers()\n",
    "tickers = sp500_tickers.index.tolist()\n",
    "\n",
    "\n",
    "# Fetch metadata from Nasdaq Data Link\n",
    "meta = nasdaqdatalink.get_table('SHARADAR/TICKERS', table='SF1', paginate=True)\n",
    "\n",
    "\n",
    "meta = meta.set_index(['ticker'])\n",
    "tickers = [t for t in tickers if t in meta.index]\n",
    "\n",
    "filtered_tickers,meta_ex_cu_de_cap = filter_universe(meta, tickers, exchange_select=['NYSE','NASDAQ','BATS'], currency_select=['USD'], delisted_select=['N'], cap_select=['6 - Mega', '5 - Large', '4 - Mid'])\n",
    "# # print(filtered_tickers)\n",
    "ohlcv_data = get_stock_data(filtered_tickers, start_date, end_date)\n",
    "second_universe = dollar_volume_universe(50, ohlcv_data, 20)\n",
    "\n",
    "\n",
    "final_universe = filter_by_sector(second_universe, meta_ex_cu_de_cap, sec_to_drop=['Financial Services',None])\n",
    "print(\"Final Universe:\", final_universe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_universe= final_universe[:5]\n",
    "print(short_universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlcv_short_universe= get_stock_data(short_universe, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= ohlcv_short_universe[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_benchmark(start, end):\n",
    "    \"\"\"Fetch S&P 500 (^GSPC) data as benchmark.\"\"\"\n",
    "    market = '^GSPC'\n",
    "    dfm = yf.download(market, start,end)\n",
    "    dfm = dfm.rename(columns={'Open': 'open', \n",
    "                              'High': 'high', \n",
    "                              'Low': 'low', \n",
    "                              'Close': 'close',\n",
    "                              'Volume': 'volume'})\n",
    "    print(dfm.head())\n",
    "    dfm.index.name = 'date'\n",
    "    benchmark = dfm['close'].pct_change()\n",
    "    benchmark.index = benchmark.index.tz_localize('UTC')\n",
    "    return benchmark,dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_data,dfm = get_benchmark(start_date, end_date)\n",
    "benchmark_data_short= benchmark_data[:6]\n",
    "print(\"Benchmark Data:\", benchmark_data_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import statsmodels.api as sm\n",
    "import yfinance as yf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.losses import Huber as huber_loss\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Improved data preparation function with better ticker handling\n",
    "def prepare_yf_data_for_modeling(data):\n",
    "    \"\"\"Convert Yahoo Finance data to flat format needed for model, ensuring proper ticker encoding\"\"\"\n",
    "    \n",
    "    # Initialize an empty DataFrame for the result\n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    # Handle Dictionary Input\n",
    "    if isinstance(data, dict):\n",
    "        if 'Close' in data and isinstance(data['Close'], pd.DataFrame):\n",
    "            # Handle dictionary with DataFrames as values (multi-ticker case)\n",
    "            tickers = data['Close'].columns\n",
    "            \n",
    "            for ticker in tickers:\n",
    "                ticker_data = pd.DataFrame()\n",
    "                for column in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
    "                    if column in data:\n",
    "                        ticker_data[column] = data[column][ticker]\n",
    "                ticker_data['Ticker'] = ticker\n",
    "                ticker_data['Date'] = ticker_data.index\n",
    "                result_df = pd.concat([result_df, ticker_data])\n",
    "        else:\n",
    "            # Handle simple dictionary (single-ticker case)\n",
    "            result_df = pd.DataFrame(data)\n",
    "            if 'Ticker' not in result_df.columns:\n",
    "                result_df['Ticker'] = 'SINGLE'\n",
    "    \n",
    "    # Handle DataFrame Input\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        # For multi-ticker data with MultiIndex columns\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            for ticker in data['Close'].columns:\n",
    "                ticker_data = pd.DataFrame()\n",
    "                ticker_data['Open'] = data['Open'][ticker]\n",
    "                ticker_data['High'] = data['High'][ticker]\n",
    "                ticker_data['Low'] = data['Low'][ticker]\n",
    "                ticker_data['Close'] = data['Close'][ticker]\n",
    "                ticker_data['Volume'] = data['Volume'][ticker]\n",
    "                ticker_data['Ticker'] = ticker\n",
    "                ticker_data['Date'] = ticker_data.index\n",
    "                result_df = pd.concat([result_df, ticker_data])\n",
    "        else:\n",
    "            # Single ticker case\n",
    "            result_df = data.copy()\n",
    "            if 'Ticker' not in result_df.columns:\n",
    "                result_df['Ticker'] = 'SINGLE'\n",
    "            if 'Date' not in result_df.columns and isinstance(result_df.index, pd.DatetimeIndex):\n",
    "                result_df = result_df.reset_index()\n",
    "                if 'index' in result_df.columns:\n",
    "                    result_df = result_df.rename(columns={'index': 'Date'})\n",
    "    else:\n",
    "        raise TypeError(f\"Expected DataFrame or dict, got {type(data)}\")\n",
    "    \n",
    "    # Create a MultiIndex with Date and Ticker\n",
    "    if 'Date' not in result_df.columns and result_df.index.name != 'Date':\n",
    "        result_df['Date'] = result_df.index\n",
    "    \n",
    "    # Create ticker encoding mapping\n",
    "    result_df['Ticker_Code'], ticker_mapping = pd.factorize(result_df['Ticker'])\n",
    "    \n",
    "    # Set MultiIndex\n",
    "    result_df = result_df.set_index(['Date', 'Ticker'])\n",
    "    \n",
    "    print(f\"Prepared data with shape: {result_df.shape}\")\n",
    "    print(f\"Columns: {result_df.columns.tolist()}\")\n",
    "    print(f\"Tickers found: {ticker_mapping.tolist()}\")\n",
    "    \n",
    "    # Store ticker mapping for later decoding\n",
    "    result_df.attrs['ticker_mapping'] = dict(enumerate(ticker_mapping))\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def get_stock_data(tickers, start, end):\n",
    "    \"\"\"Fetch OHLCV data from Yahoo Finance, ensuring consistent DataFrame output\"\"\"\n",
    "    data = yf.download(tickers, start=start, end=end)\n",
    "    \n",
    "    # Handle single ticker case\n",
    "    if isinstance(data, pd.DataFrame) and not isinstance(data.columns, pd.MultiIndex):\n",
    "        ticker = tickers if isinstance(tickers, str) else tickers[0]\n",
    "        single_data = data.copy()\n",
    "        single_data['Ticker'] = ticker\n",
    "        single_data['Ticker_Code'] = 0  # Single ticker gets code 0\n",
    "        return single_data\n",
    "    \n",
    "    return data\n",
    "\n",
    "def calculate_indicators(df):\n",
    "    \"\"\"Calculate technical indicators for each ticker separately\"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Check for required column\n",
    "    if 'Close' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'Close' column\")\n",
    "    \n",
    "    # Iterate through each ticker\n",
    "    for ticker in df.index.get_level_values('Ticker').unique():\n",
    "        # Get data for this ticker\n",
    "        ticker_data = df.xs(ticker, level='Ticker')\n",
    "        \n",
    "        # Calculate indicators\n",
    "        result_df.loc[(slice(None), ticker), 'MA_50'] = ticker_data['Close'].rolling(window=min(50, len(ticker_data))).mean().values\n",
    "        result_df.loc[(slice(None), ticker), 'RSI'] = calculate_rsi(ticker_data['Close'], period=min(14, len(ticker_data)-1)).values\n",
    "        result_df.loc[(slice(None), ticker), 'MACD'] = calculate_macd(ticker_data['Close']).values\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def calculate_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=period).mean()\n",
    "    avg_loss = loss.rolling(window=period).mean()\n",
    "    # Avoid division by zero\n",
    "    rs = avg_gain / avg_loss.replace(0, np.nan).fillna(0.00001)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_macd(series, short_window=12, long_window=26, signal_window=9):\n",
    "    short_ema = series.ewm(span=short_window, adjust=False).mean()\n",
    "    long_ema = series.ewm(span=long_window, adjust=False).mean()\n",
    "    macd = short_ema - long_ema\n",
    "    signal = macd.ewm(span=signal_window, adjust=False).mean()\n",
    "    return macd - signal\n",
    "\n",
    "def prepare_data(df, look_back=5):\n",
    "    \"\"\"Prepare data for LSTM model with ticker encoding\"\"\"\n",
    "    # Calculate next day returns for each ticker\n",
    "    next_day_returns = pd.Series(index=df.index)\n",
    "    \n",
    "    for ticker in df.index.get_level_values('Ticker').unique():\n",
    "        ticker_data = df.xs(ticker, level='Ticker')\n",
    "        returns = ticker_data['Close'].pct_change().shift(-1) * 100\n",
    "        next_day_returns.loc[(slice(None), ticker)] = returns.values\n",
    "    \n",
    "    df['Next Day Return'] = next_day_returns\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"Error: No data available after calculating indicators and returns\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    features = ['Open', 'Close', 'MA_50', 'RSI', 'MACD']\n",
    "    X = []\n",
    "    y = []\n",
    "    ticker_codes = []\n",
    "    tickers = []\n",
    "    \n",
    "    # Iterate through each ticker\n",
    "    for ticker in df.index.get_level_values('Ticker').unique():\n",
    "        ticker_df = df.xs(ticker, level='Ticker')\n",
    "        \n",
    "        if len(ticker_df) <= look_back:\n",
    "            print(f\"Warning: Not enough data points for ticker {ticker}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Ensure all features are available\n",
    "            if not all(feature in ticker_df.columns for feature in features):\n",
    "                print(f\"Warning: Missing features for ticker {ticker}. Skipping.\")\n",
    "                continue\n",
    "                \n",
    "            if ticker_df[features].isna().all().any():\n",
    "                print(f\"Warning: All NaN values in at least one feature for ticker {ticker}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Get ticker code from first row\n",
    "            ticker_code = df.xs(ticker, level='Ticker')['Ticker_Code'].iloc[0]\n",
    "            \n",
    "            ticker_features = ticker_df[features].values\n",
    "            ticker_target = ticker_df['Next Day Return'].values\n",
    "            \n",
    "            for i in range(look_back, len(ticker_features)):\n",
    "                # Check for NaN values in this window\n",
    "                window = ticker_features[i-look_back:i]\n",
    "                if np.isnan(window).any() or np.isnan(ticker_target[i]):\n",
    "                    continue\n",
    "                    \n",
    "                X.append(window)\n",
    "                y.append(ticker_target[i])\n",
    "                ticker_codes.append(ticker_code)\n",
    "                tickers.append(ticker)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing ticker {ticker}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"Error: No data points generated after filtering\")\n",
    "        return None, None, None, None\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    ticker_codes = np.array(ticker_codes)\n",
    "    tickers = np.array(tickers)\n",
    "    \n",
    "    print(f\"Prepared data with shapes X:{X.shape}, y:{y.shape}\")\n",
    "    \n",
    "    return X, y, ticker_codes, tickers\n",
    "\n",
    "\n",
    "def train_lstm_model(X_train, y_train, epochs=300, batch_size=32):\n",
    "    \"\"\"Train an enhanced LSTM model for stock price prediction with advanced architecture\"\"\"\n",
    "    if X_train is None or y_train is None:\n",
    "        print(\"Error: No training data available\")\n",
    "        return None\n",
    "    if len(X_train) == 0 or len(y_train) == 0:\n",
    "        print(\"Error: Empty training data\")\n",
    "        return None\n",
    "    if len(X_train.shape) != 3:\n",
    "        print(f\"Error: Expected 3D array for X_train, got shape {X_train.shape}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Data normalization\n",
    "        mean = X_train.mean(axis=0)\n",
    "        std = X_train.std(axis=0)\n",
    "        X_train_normalized = (X_train - mean) / (std + 1e-8)\n",
    "\n",
    "        model = Sequential([\n",
    "            # Bidirectional LSTM layers for better pattern recognition\n",
    "            Bidirectional(LSTM(100, return_sequences=True, \n",
    "                             kernel_regularizer=l2(1e-5),\n",
    "                             recurrent_regularizer=l2(1e-5),\n",
    "                             dropout=0.2,\n",
    "                             recurrent_dropout=0.2), \n",
    "                         input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            \n",
    "            BatchNormalization(),\n",
    "            \n",
    "            Bidirectional(LSTM(50, return_sequences=True,\n",
    "                             kernel_regularizer=l2(1e-5),\n",
    "                             recurrent_regularizer=l2(1e-5),\n",
    "                             dropout=0.2,\n",
    "                             recurrent_dropout=0.2)),\n",
    "            \n",
    "            BatchNormalization(),\n",
    "            \n",
    "            Bidirectional(LSTM(25, return_sequences=False,\n",
    "                             kernel_regularizer=l2(1e-5),\n",
    "                             recurrent_regularizer=l2(1e-5),\n",
    "                             dropout=0.2,\n",
    "                             recurrent_dropout=0.2)),\n",
    "            \n",
    "            BatchNormalization(),\n",
    "            \n",
    "            Dense(50, activation='relu', kernel_regularizer=l2(1e-5)),\n",
    "            Dropout(0.2),\n",
    "            \n",
    "            Dense(25, activation='relu', kernel_regularizer=l2(1e-5)),\n",
    "            Dropout(0.2),\n",
    "            \n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "\n",
    "        optimizer = Adam(learning_rate=0.001)\n",
    "        \n",
    "        model.compile(optimizer=optimizer, \n",
    "                     loss=huber_loss,\n",
    "                     metrics=['mae', 'mse'])\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "\n",
    "        # Train with validation split\n",
    "        history = model.fit(\n",
    "            X_train_normalized, \n",
    "            y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Store normalization parameters as model attributes\n",
    "        model.mean = mean\n",
    "        model.std = std\n",
    "        \n",
    "        return model  # Return only the model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_signals_and_weights(predictions, ticker_codes, tickers, ticker_mapping):\n",
    "    \"\"\"Generate trading signals and portfolio weights with improved risk management\"\"\"\n",
    "    try:\n",
    "        # Create a dictionary to map predictions to tickers\n",
    "        ticker_predictions = defaultdict(list)\n",
    "        for i, (ticker, pred) in enumerate(zip(tickers, predictions)):\n",
    "            ticker_predictions[ticker].append(pred)\n",
    "\n",
    "        # Enhanced portfolio data generation with additional metrics\n",
    "        portfolio_data = []\n",
    "        for ticker, preds in ticker_predictions.items():\n",
    "            avg_prediction = np.mean(preds)\n",
    "            pred_std = np.std(preds)  # Measure of prediction uncertainty\n",
    "            \n",
    "            # Enhanced signal generation with confidence threshold\n",
    "            confidence_threshold = 0.5 * pred_std\n",
    "            if avg_prediction > confidence_threshold:\n",
    "                signal = 'BUY'\n",
    "            elif avg_prediction < -confidence_threshold:\n",
    "                signal = 'SELL'\n",
    "            else:\n",
    "                signal = 'HOLD'\n",
    "\n",
    "            # Find ticker code\n",
    "            idx = np.where(tickers == ticker)[0][0]\n",
    "            ticker_code = ticker_codes[idx]\n",
    "\n",
    "            # Calculate prediction confidence score\n",
    "            confidence_score = abs(avg_prediction) / (pred_std + 1e-8)\n",
    "            \n",
    "            portfolio_data.append({\n",
    "                'Ticker_Code': ticker_code,\n",
    "                'Ticker': ticker,\n",
    "                'Predicted_Return': avg_prediction,\n",
    "                'Prediction_Std': pred_std,\n",
    "                'Confidence_Score': confidence_score,\n",
    "                'Signal': signal,\n",
    "                'Prediction_Count': len(preds)\n",
    "            })\n",
    "\n",
    "        portfolio = pd.DataFrame(portfolio_data)\n",
    "        \n",
    "        if len(portfolio) == 0:\n",
    "            raise ValueError(\"No portfolio data generated\")\n",
    "\n",
    "        # Enhanced weight calculation incorporating confidence scores\n",
    "        portfolio['Abs_Return'] = np.abs(portfolio['Predicted_Return'])\n",
    "        portfolio['Risk_Adjusted_Return'] = portfolio['Abs_Return'] * portfolio['Confidence_Score']\n",
    "        \n",
    "        total_risk_adj_return = portfolio['Risk_Adjusted_Return'].sum()\n",
    "        \n",
    "        if total_risk_adj_return > 0:\n",
    "            portfolio['Raw_Weight'] = portfolio['Risk_Adjusted_Return'] / total_risk_adj_return\n",
    "            \n",
    "            # Position sizing with risk management\n",
    "            max_position_size = 0.25  # Maximum 25% in any single position\n",
    "            portfolio['Raw_Weight'] = portfolio['Raw_Weight'].clip(upper=max_position_size)\n",
    "            \n",
    "            # Recalculate weights after clipping\n",
    "            portfolio['Raw_Weight'] = portfolio['Raw_Weight'] / portfolio['Raw_Weight'].sum()\n",
    "            \n",
    "            portfolio['Weight'] = np.where(\n",
    "                portfolio['Signal'] == 'BUY',\n",
    "                portfolio['Raw_Weight'] * 100,\n",
    "                np.where(\n",
    "                    portfolio['Signal'] == 'SELL',\n",
    "                    -portfolio['Raw_Weight'] * 100,\n",
    "                    0\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # Modified equal weighting with risk management\n",
    "            max_positions = min(len(portfolio), 10)  # Maximum number of positions\n",
    "            weight = 100 / max_positions\n",
    "            \n",
    "            # Sort by confidence score and take top positions\n",
    "            portfolio = portfolio.sort_values('Confidence_Score', ascending=False)\n",
    "            portfolio['Weight'] = np.where(\n",
    "                portfolio.index < max_positions,\n",
    "                np.where(\n",
    "                    portfolio['Signal'] == 'BUY', weight,\n",
    "                    np.where(portfolio['Signal'] == 'SELL', -weight, 0)\n",
    "                ),\n",
    "                0\n",
    "            )\n",
    "\n",
    "        # Keep original columns for output consistency\n",
    "        portfolio = portfolio[['Ticker_Code', 'Ticker', 'Predicted_Return', 'Signal', 'Weight']]\n",
    "        return portfolio\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in signal generation: {e}\")\n",
    "        return None\n",
    "\n",
    "def backtest_portfolio(portfolio, benchmark_df, stock_data, initial_capital=10000):\n",
    "    \"\"\"Backtest portfolio performance against a benchmark with improved calculations\"\"\"\n",
    "    try:\n",
    "        tickers = portfolio['Ticker'].values\n",
    "        weights = portfolio['Weight'].values / 100  # Convert weights to decimals\n",
    "        \n",
    "        # Create DataFrame for prices\n",
    "        df_prices = pd.DataFrame(index=benchmark_df.index)\n",
    "        df_prices['Benchmark'] = benchmark_df['close']\n",
    "        \n",
    "        # Get stock prices and align with benchmark dates\n",
    "        for ticker in tickers:\n",
    "            ticker_data = stock_data.xs(ticker, level='Ticker')['Close']\n",
    "            df_prices[ticker] = ticker_data.reindex(benchmark_df.index)\n",
    "        \n",
    "        # Remove any dates with missing data\n",
    "        df_prices = df_prices.dropna()\n",
    "        \n",
    "        if len(df_prices) == 0:\n",
    "            raise ValueError(\"No overlapping dates found between stocks and benchmark\")\n",
    "        \n",
    "        # Calculate daily returns\n",
    "        daily_returns = df_prices[tickers].pct_change()\n",
    "        \n",
    "        # Initialize portfolio value array\n",
    "        portfolio_values = np.zeros(len(df_prices))\n",
    "        portfolio_values[0] = initial_capital\n",
    "        \n",
    "        # Calculate portfolio value each day\n",
    "        for i in range(1, len(df_prices)):\n",
    "            # Calculate daily portfolio return\n",
    "            day_return = np.sum(daily_returns.iloc[i] * weights)\n",
    "            # Update portfolio value\n",
    "            portfolio_values[i] = portfolio_values[i-1] * (1 + day_return)\n",
    "        \n",
    "        df_prices['Portfolio'] = portfolio_values\n",
    "        \n",
    "        return df_prices\n",
    "    except Exception as e:\n",
    "        print(f\"Error in backtesting: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_performance(df_prices, initial_capital=10000, risk_free_rate=0.02):\n",
    "    \"\"\"Calculate portfolio performance metrics including benchmark comparison\"\"\"\n",
    "    try:\n",
    "        # Calculate daily returns\n",
    "        portfolio_daily_returns = df_prices['Portfolio'].pct_change().dropna()\n",
    "        benchmark_daily_returns = df_prices['Benchmark'].pct_change().dropna()\n",
    "        print(df_prices['Portfolio'])\n",
    "        print(df_prices['Benchmark'])\n",
    "        # Basic metrics\n",
    "        portfolio_total_return = ((df_prices['Portfolio'].iloc[-1] / initial_capital) - 1) * 100\n",
    "        benchmark_total_return = ((df_prices['Benchmark'].iloc[-1] / df_prices['Benchmark'].iloc[0]) - 1) * 100\n",
    "        \n",
    "        # Time period in years\n",
    "        days = len(df_prices)\n",
    "        years = days / 252\n",
    "        \n",
    "        # CAGR\n",
    "        portfolio_cagr = (((df_prices['Portfolio'].iloc[-1] / initial_capital) ** (1/years)) - 1) * 100\n",
    "        benchmark_cagr = (((df_prices['Benchmark'].iloc[-1] / df_prices['Benchmark'].iloc[0]) ** (1/years)) - 1) * 100\n",
    "        \n",
    "        # Risk metrics\n",
    "        portfolio_std = portfolio_daily_returns.std() * np.sqrt(252)\n",
    "        excess_returns = portfolio_daily_returns - (risk_free_rate/252)\n",
    "        \n",
    "        # Sharpe Ratio\n",
    "        sharpe = (np.mean(portfolio_daily_returns) * 252 - risk_free_rate) / portfolio_std\n",
    "        \n",
    "        # Maximum Drawdown\n",
    "        rolling_max = df_prices['Portfolio'].cummax()\n",
    "        drawdowns = (df_prices['Portfolio'] - rolling_max) / rolling_max\n",
    "        max_drawdown = drawdowns.min() * 100\n",
    "        \n",
    "        # Win Rate\n",
    "        win_rate = (portfolio_daily_returns > 0).mean() * 100\n",
    "        \n",
    "        # Beta and Alpha\n",
    "        covariance = np.cov(portfolio_daily_returns, benchmark_daily_returns, ddof=0)[0,1]\n",
    "        variance = np.var(benchmark_daily_returns, ddof=0)\n",
    "        beta = covariance / variance\n",
    "        \n",
    "        expected_return = risk_free_rate + beta * (np.mean(benchmark_daily_returns) * 252 - risk_free_rate)\n",
    "        actual_return = np.mean(portfolio_daily_returns) * 252\n",
    "        alpha = (actual_return - expected_return) * 100\n",
    "        \n",
    "        metrics = {\n",
    "            'portfolio_total_return': portfolio_total_return,\n",
    "            'benchmark_total_return': benchmark_total_return,\n",
    "            'portfolio_cagr': portfolio_cagr,\n",
    "            'benchmark_cagr': benchmark_cagr,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'win_rate': win_rate,\n",
    "            'alpha': alpha,\n",
    "            'beta': beta\n",
    "        }\n",
    "        \n",
    "        # Print metrics\n",
    "        print(\"\\nðŸ“Š PORTFOLIO PERFORMANCE REPORT ðŸ“Š\")\n",
    "        print(f\"Portfolio Total Return: {portfolio_total_return:.2f}%\")\n",
    "        print(f\"Benchmark Total Return: {benchmark_total_return:.2f}%\")\n",
    "        print(f\"Portfolio CAGR: {portfolio_cagr:.2f}%\")\n",
    "        print(f\"Benchmark CAGR: {benchmark_cagr:.2f}%\")\n",
    "        print(f\"Max Drawdown: {max_drawdown:.2f}%\")\n",
    "        print(f\"Sharpe Ratio: {sharpe:.2f}\")\n",
    "        print(f\"Win Rate: {win_rate:.2f}%\")\n",
    "        print(f\"Alpha: {alpha:.2f}%\")\n",
    "        print(f\"Beta: {beta:.2f}\")\n",
    "        \n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(df_prices.index, df_prices['Portfolio'], label='Portfolio', linewidth=2)\n",
    "        plt.plot(df_prices.index, \n",
    "                df_prices['Benchmark'] * (df_prices['Portfolio'].iloc[0] / df_prices['Benchmark'].iloc[0]),\n",
    "                label='Benchmark (Normalized)', \n",
    "                linestyle='--')\n",
    "        plt.title('Portfolio vs Benchmark Performance')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Value ($)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in performance evaluation: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_alpha_generation(tickers, start_date, end_date, data=None, benchmark_data_short=None, dfm=None):\n",
    "    \"\"\"Main function to run the alpha generation pipeline with explicit variable passing\"\"\"\n",
    "    try:\n",
    "        # 1. Get the data if not provided\n",
    "        if data is None:\n",
    "            stock_data = get_stock_data(tickers, start_date, end_date)\n",
    "        else:\n",
    "            stock_data = data\n",
    "            \n",
    "        if benchmark_data_short is None or dfm is None:\n",
    "            # Fetch benchmark data if not provided\n",
    "            benchmark_data_short = yf.download('^GSPC', start=start_date, end=end_date)\n",
    "            dfm = benchmark_data_short.reset_index().rename(columns={'Date': 'date', 'Close': 'close'}).set_index('date')\n",
    "        else:\n",
    "            benchmark_data_short = benchmark_data_short\n",
    "            dfm = dfm\n",
    "        \n",
    "        # 2. Prepare the data for modeling\n",
    "        prepared_data = prepare_yf_data_for_modeling(stock_data)\n",
    "        prepared_data = calculate_indicators(prepared_data)\n",
    "        \n",
    "        # Get ticker mapping from prepared data\n",
    "        ticker_mapping = prepared_data.attrs.get('ticker_mapping', {})\n",
    "        \n",
    "        # 3. Prepare data for LSTM\n",
    "        X, y, ticker_codes, tickers_array = prepare_data(prepared_data)\n",
    "        \n",
    "        if X is not None and len(X) > 0:\n",
    "            # 4. Train the model\n",
    "            model = train_lstm_model(X, y)\n",
    "            \n",
    "            if model is not None:\n",
    "                # 5. Generate predictions and create portfolio\n",
    "                try:\n",
    "                    predictions = model.predict(X).flatten()\n",
    "                    portfolio = generate_signals_and_weights(predictions, ticker_codes, tickers_array, ticker_mapping)\n",
    "                    \n",
    "                    # Decode ticker codes to show actual ticker names\n",
    "                    print(\"\\nðŸ“Š Portfolio Construction:\")\n",
    "                    print(portfolio)\n",
    "                    \n",
    "                    # 6. Backtest the portfolio\n",
    "                    df_prices = backtest_portfolio(portfolio, dfm, prepared_data)\n",
    "                    \n",
    "                    # 7. Evaluate performance\n",
    "                    initial_capital = 1000\n",
    "                    performance_metrics = evaluate_performance(df_prices, initial_capital)\n",
    "                    \n",
    "                    return portfolio, df_prices, performance_metrics\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during portfolio construction or backtesting: {e}\")\n",
    "                    return None, None, None\n",
    "            else:\n",
    "                print(\"Model training failed. Cannot proceed with portfolio construction.\")\n",
    "                return None, None, None\n",
    "        else:\n",
    "            print(\"Data preparation failed. Cannot proceed with model training.\")\n",
    "            return None, None, None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in alpha generation pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers= short_universe\n",
    "portfolio, performance_data, metrics = run_alpha_generation(tickers, start_date, end_date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
